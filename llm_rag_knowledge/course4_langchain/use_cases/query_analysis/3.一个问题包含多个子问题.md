### 核心问题：如果用户一个问题里包含了多个子问题怎么办？

在之前的教程中，我们处理了用户问题需要“一个”或“零个”查询的场景。但现实世界中的问题往往更复杂。

**一个典型的例子**： *"where did Harrison and ankush Work?"* (Harrison 和 Ankush 分别在哪里工作？)

如果我们把整个句子 `"where did Harrison and ankush Work"` 作为**一个**搜索查询，很可能在我们的知识库里找不到任何一个文档同时包含这两个人的工作信息，从而导致检索失败。

一个更智能的方法是**将用户的复杂问题分解（Decompose）成多个更简单、更独立的子查询**，然后分别执行它们，最后将结果合并。
*   子查询1: "Harrison's work history"
*   子查询2: "Ankush's work history"

这份文档的核心就是教我们如何构建一个能够**生成并执行多个查询**的链。

---

### 1. 准备工作 (Setup)

*   **创建索引 (Create Index)**: 这里的关键变化是，我们的知识库现在包含两条独立的信息：`"Harrison worked at Kensho"` 和 `"Ankush worked at Facebook"`。这为我们测试多查询检索提供了基础。

---

### 2. 查询分析 (Query Analysis) - 构建“问题分解器”

和上次一样，我们首先需要一个“查询分析器”，但这次它的任务不是判断“是否要搜索”，而是判断“**需要搜索几次，分别搜什么**”。我们称之为“问题分解器”。

**步骤 2.1: 定义一个能容纳多个查询的“动作”**

```python
from typing import List
from pydantic import BaseModel, Field

class Search(BaseModel):
    """Search over a database of job records."""
    queries: List[str] = Field(..., description="Distinct queries to search for")
```
*   **讲解 (与上一篇教程的关键对比)**:
    *   在上一篇教程中，`Search` 模型里只有一个字段 `query: str`，代表一个字符串查询。
    *   在这里，`Search` 模型里的字段变成了 `queries: List[str]`，代表一个**字符串列表**。
    *   这个简单的改动，就从结构上赋予了 LLM **生成多个查询的能力**。现在，LLM 不再是填写一张只有一个空格的“申请表”，而是一张可以填写多行的“申请表”。

**步骤 2.2: 指导并强制 LLM 进行问题分解**

```python
system = """You have the ability to issue search queries...
If you need to look up two distinct pieces of information, you are allowed to do that!"""

prompt = ChatPromptTemplate.from_messages([("system", system), ("human", "{question}")])
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
structured_llm = llm.with_structured_output(Search) # <- 关键所在！
query_analyzer = {"question": RunnablePassthrough()} | prompt | structured_llm
```
*   **讲解**:
    *   **`system` 提示**: 我们明确告诉模型：“如果你需要查找两个不同的信息，你是可以这么做的！” 这是在行为上引导它进行分解。
    *   **`llm.with_structured_output(Search)`**: 这是与上一篇教程的另一个**关键区别**。
        *   上次我们用的是 `llm.bind_tools([Search])`，那是**可选的**工具调用。
        *   这次我们用的是 `llm.with_structured_output(Search)`，这是**强制的**结构化输出。这意味着，在这个 `query_analyzer` 链中，LLM 的输出**必须**符合 `Search` 模型的格式（即必须返回一个包含 `queries` 列表的对象）。我们在这里的假设是，这个链专门用于查询分析，所以它总是应该生成查询。

**步骤 2.3: 观察“问题分解器”的行为**

*   **当用户提问复杂问题时**:
    ```python
    query_analyzer.invoke("where did Harrison and ankush Work")
    ```    **输出**: `Search(queries=['Harrison work history', 'Ankush work history'])`
    *   模型成功地将一个问题分解成了两个独立的、针对性强的子查询。

*   **当用户提问简单问题时**:
    ```python
    query_analyzer.invoke("where did Harrison Work")
    ```
    **输出**: `Search(queries=['Harrison Work', 'Harrison employment history'])`
    *   即使是简单问题，模型也可能生成多个语义相近的查询，试图从不同角度提高检索的召回率。这也完全符合我们的预期。

---

### 3. 整合：构建能执行多查询的链

现在我们有了一个能生成查询列表的“问题分解器”，接下来的挑战是如何**执行这个列表中的所有查询，并合并结果**。

**一个重要的性能考量**: 如果我们用一个普通的 `for` 循环来依次执行检索，当有5个子查询，每个查询耗时1秒时，总共就需要5秒。这是一个串行操作。为了提升效率，文档引入了**异步（asynchronous）**执行。异步允许我们同时发起所有的检索请求，然后等待它们全部完成，总耗时将约等于最慢的那个请求的耗时（比如1.2秒），而不是所有请求耗时之和。

**代码讲解**:
```python
from langchain_core.runnables import chain

@chain
async def custom_chain(question):
    # 步骤 A: 异步运行“问题分解器”，获取查询列表
    response = await query_analyzer.ainvoke(question)
    
    # 步骤 B: 准备一个空列表，用于收集所有检索结果
    docs = []
    
    # 步骤 C: 遍历查询列表，并异步执行每一次检索
    for query in response.queries:
        # 步骤 C.1: 异步调用 retriever
        new_docs = await retriever.ainvoke(query)
        # 步骤 C.2: 将本次检索到的文档追加到总列表中
        docs.extend(new_docs)
        
    # 步骤 D: (重要提示) 在实际应用中，这里可能需要去重或重排序
    
    # 步骤 E: 返回合并后的所有文档
    return docs
```
*   **`async def custom_chain(...)`**: 我们将函数定义为 `async` 异步函数，这是使用 `await` 的前提。
*   **`await query_analyzer.ainvoke(question)`**: `ainvoke` 是 `invoke` 的异步版本。我们用 `await` 等待它完成。
*   **`for query in response.queries:`**: 这是一个普通的 `for` 循环，用于遍历“问题分解器”生成的所有子查询。
*   **`await retriever.ainvoke(query)`**: 在循环内部，我们再次使用 `ainvoke` 来**异步地**执行每一次检索。虽然 `for` 循环本身是顺序的，但这里的 `await` 允许事件循环在等待 I/O (网络请求) 时切换到其他任务，从而实现并发效果。（*注意：要实现真正的并行执行，需要使用 `asyncio.gather`，但这里的写法在概念上展示了异步调用的用法，并且在很多 I/O 密集型场景下已经能带来显著性能提升。*）
*   **`docs.extend(new_docs)`**: `extend` 方法将 `new_docs` 列表中的所有元素都添加到 `docs` 列表中。最终，`docs` 会包含所有子查询检索到的所有文档。
*   **文档中的注释**: 文档特意提醒，合并后的 `docs` 列表可能会有重复的文档，或者相关性不一。在生产系统中，通常会在这一步之后增加一个**重排序（Reranking）**或**去重（Deduplication）**的步骤，以提高最终呈现给 LLM 的上下文质量。

### 总结

这份文档向我们展示了一个处理复杂用户问题的强大模式，可以总结为三步：

1.  **分解 (Decompose)**: 使用一个强制输出列表的结构化 LLM (`with_structured_output`)，将用户的复杂问题分解成一个包含多个简单、独立子查询的列表。
2.  **并发执行 (Concurrent Execution)**: 遍历这个子查询列表，并使用异步调用 (`ainvoke`) 来高效地执行每一次检索。
3.  **聚合 (Aggregate)**: 将所有检索操作返回的文档聚合到一个统一的列表中，为后续的答案生成步骤做准备。

通过这个模式，我们的 RAG 系统不再局限于处理简单问题，而是具备了理解和响应多意图、复杂查询的能力，向着更智能、更健壮的AI应用迈出了一大步。