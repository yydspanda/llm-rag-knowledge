
### 核心概念：什么是聊天机器人的记忆？

聊天机器人的记忆，指的是它能够利用之前对话内容作为上下文的能力。没有记忆的机器人，每一次对话都是孤立的，它不记得你上一秒钟说了什么。记忆让对话能够连贯地进行下去。

文档中提到了几种实现记忆的方式：
1.  **简单填充**: 将之前的对话消息全部塞进下一次的提示（Prompt）里。
2.  **裁剪消息**: 像上面一样，但在塞入前先裁剪掉一些旧消息，以防超出模型的上下文长度限制。
3.  **复杂修改**: 比如为长对话生成摘要，用摘要来代替冗长的历史记录。

**一个重要的提示**: 文档明确指出，从 LangChain v0.3 版本开始，官方**推荐使用 LangGraph 的持久化功能**来为应用添加记忆。旧的 `RunnableWithMessageHistory` 方法虽然还能用，但新项目建议使用 LangGraph。这篇指南讲解的就是最新的 LangGraph 方法。

---

### 1. 基础：手动传递消息 (Message passing)

这是最基本、最原始的记忆形式，即由我们自己手动管理和传递历史消息。

**代码讲解**:
```python
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# 1. 创建提示模板
prompt = ChatPromptTemplate.from_messages(
    [
        # 系统指令，设定机器人的角色
        SystemMessage(
            content="You are a helpful assistant. Answer all questions to the best of your ability."
        ),
        # 一个占位符，我们将在这里手动插入历史消息
        MessagesPlaceholder(variable_name="messages"),
    ]
)

chain = prompt | model

# 2. 手动构建历史消息并发起调用
ai_msg = chain.invoke(
    {
        "messages": [
            HumanMessage(content="Translate from English to French: I love programming."),
            AIMessage(content="J'adore la programmation."),
            HumanMessage(content="What did you just say?"), # 这是当前的问题
        ],
    }
)
```

*   **`MessagesPlaceholder(variable_name="messages")`**: 这是关键。它在提示模板中开辟了一个“槽位”，专门用来接收一个消息列表（`List[BaseMessage]`）。
*   **`chain.invoke`**: 在调用链时，我们手动创建了一个 `messages` 列表。这个列表包含了完整的对话历史：用户的第一个问题、AI 的回答，以及用户的第二个问题。
*   **结果**: 模型接收到了完整的上下文，因此它能理解 "What did you just say?" 指的是上一句法语，并正确回答。

**小结**: 这种方法有效，但缺点是**需要我们自己在程序中维护这个 `messages` 列表**。每次对话后，都需要手动将新的问答追加进去，非常繁琐。

---

### 2. 推荐方法：自动历史管理 (Automatic history management)

这里介绍了 LangChain 推荐的、使用 LangGraph 持久化功能来实现自动记忆的方法。核心思想是使用一个“检查点（checkpointer）”来自动保存和加载对话状态。

**代码讲解**:
```python
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph

# 1. 创建一个内存检查点
memory = MemorySaver()

# 2. 定义 LangGraph 的工作流
workflow = StateGraph(state_schema=MessagesState)

# 3. 定义一个节点，这个节点负责调用模型
def call_model(state: MessagesState):
    system_prompt = "..."
    # 从状态中获取所有历史消息
    messages = [SystemMessage(content=system_prompt)] + state["messages"]
    response = model.invoke(messages)
    # 返回新的消息，LangGraph 会自动将其追加到状态中
    return {"messages": response}

# 4. 将节点和边添加到工作流中
workflow.add_node("model", call_model)
workflow.add_edge(START, "model") # 从起点直接连接到 model 节点

# 5. 编译应用，并传入检查点
app = workflow.compile(checkpointer=memory)
```
*   **`MemorySaver()`**: 这是一个最简单的检查点，它将对话历史保存在程序的内存里。对于生产环境，可以替换成 Redis、Postgres 等持久化存储。
*   **`StateGraph(state_schema=MessagesState)`**: LangGraph 是围绕“状态”工作的。`MessagesState` 是一个内置的、专门用来存储消息列表的状态。
*   **`call_model(state: MessagesState)`**: 这是图中的一个节点。它接收当前的状态（包含了到目前为止的所有消息），调用模型，然后返回一个包含新消息的字典。LangGraph 会自动将这个新消息合并回状态中。
*   **`app = workflow.compile(checkpointer=memory)`**: 这是最关键的一步。通过提供 `checkpointer`，我们就为这个 LangGraph 应用启用了记忆功能。

**如何使用带记忆的应用**:
```python
# 第一次调用
app.invoke(
    {"messages": [HumanMessage(content="Translate to French: I love programming.")]},
    config={"configurable": {"thread_id": "1"}}, # 指定一个对话 ID
)

# 第二次调用，注意我们只传入了新的问题
app.invoke(
    {"messages": [HumanMessage(content="What did I just ask you?")]},
    config={"configurable": {"thread_id": "1"}}, # 使用相同的对话 ID
)
```
*   **`config={"configurable": {"thread_id": "1"}}`**: 这是启用记忆的关键。`thread_id` 是一个唯一的对话标识符。
    *   当 `app.invoke` 被调用时，`checkpointer` 会根据 `thread_id` 去查找之前有没有保存过的历史记录。
    *   第一次调用时，没有历史，它就直接执行。执行完毕后，`checkpointer` 会将这次的对话（1个 HumanMessage + 1个 AIMessage）以 `thread_id="1"` 为 key 保存起来。
    *   第二次调用时，`checkpointer` 发现 `thread_id="1"` 已经有历史了，它会先**自动加载**这些历史，然后连同这次的新问题一起执行。执行完毕后，再把**更新后的完整历史**保存回去。

**小结**: 我们不再需要手动管理消息列表。LangGraph 和 `checkpointer` 替我们完成了所有加载和保存的工作，我们只需要为每个独立的对话会话提供一个唯一的 `thread_id` 即可。

---

### 3. 高级技巧：修改聊天记录 (Modifying chat history)

在自动管理的基础上，我们还可以对历史记录进行“预处理”，以应对更复杂的场景。

#### 3.1. 裁剪消息 (Trimming messages)

**问题**: 模型的上下文窗口有限，如果对话太长，会报错或效率降低。
**解决**: 在将历史消息发送给模型之前，先裁剪掉一部分。

**代码讲解**:
```python
from langchain_core.messages import trim_messages

# 1. 定义一个裁剪器
trimmer = trim_messages(
    strategy="last",      # 裁剪策略：只保留最后的
    max_tokens=2,         # 保留的消息数量
    token_counter=len,    # 一个简化的计算方式：每个消息算1个token
)

# 2. 在 call_model 函数中应用裁剪器
def call_model(state: MessagesState):
    # 在调用模型前，先用 trimmer 处理历史消息
    trimmed_messages = trimmer.invoke(state["messages"])
    system_prompt = "..."
    messages = [SystemMessage(content=system_prompt)] + trimmed_messages
    response = model.invoke(messages)
    return {"messages": response}
# ... 后续编译步骤和之前一样 ...
```
*   **`trim_messages`**: 这是一个 LangChain 提供的工具函数。
*   **修改 `call_model`**: 唯一的改动就是在调用 `model.invoke` 之前，先用 `trimmer` 对 `state["messages"]` 进行处理，得到一个裁剪后的 `trimmed_messages` 列表。
*   **结果**: 当你传入一个包含4条消息的历史记录时，`trimmer` 会只保留最后2条。因此，如果“我叫Nemo”这句话是在前2条消息里，那么模型在回答“我叫什么名字？”时将无法获取该信息，因为它已经被裁剪掉了。

#### 3.2. 摘要记忆 (Summary memory)

**问题**: 简单地裁剪消息可能会丢失重要信息（比如用户的名字）。
**解决**: 与其丢弃旧消息，不如用一个 LLM 将它们**总结**成一条摘要消息，用摘要来代替冗长的历史。

**代码讲解 (这是最复杂的部分)**:
```python
def call_model(state: MessagesState):
    # ...
    message_history = state["messages"][:-1] # 获取除最新问题外的所有历史
    
    # 1. 判断是否需要生成摘要
    if len(message_history) >= 4:
        last_human_message = state["messages"][-1]
        
        # 2. 额外调用一次 LLM 来生成摘要
        summary_prompt = "Distill the above chat messages into a single summary..."
        summary_message = model.invoke(message_history + [HumanMessage(content=summary_prompt)])
        
        # 3. 告诉 checkpointer 删除所有旧消息
        delete_messages = [RemoveMessage(id=m.id) for m in state["messages"]]
        
        # 4. 用 摘要 + 最新问题 来调用模型，获取最终回复
        human_message = HumanMessage(content=last_human_message.content)
        response = model.invoke([system_message, summary_message, human_message])
        
        # 5. 构造最终要保存到 checkpointer 的状态
        # 新状态 = 摘要 + 最新问题 + 最新回复 + 删除所有旧消息的指令
        message_updates = [summary_message, human_message, response] + delete_messages
    else:
        # 如果历史不够长，就正常调用
        message_updates = model.invoke([system_message] + state["messages"])

    return {"messages": message_updates}
```
*   **核心逻辑**: 当对话历史达到一定长度（这里是4条）时：
    1.  它会触发一个“总结”操作。
    2.  它会用模型把旧的对话历史（不包括用户当前的问题）压缩成一条 `summary_message`。
    3.  然后，它不再把完整的历史传给模型，而是只传 `[系统指令, 摘要, 用户的当前问题]`。这样既保留了历史关键信息，又大大缩短了上下文长度。
    4.  `RemoveMessage` 是一个特殊的指令，它告诉 `checkpointer`：“请从历史记录中删除这些ID的消息”。
    5.  最终，checkpointer 会执行这个更新：删除所有旧消息，并存入新的三条消息（摘要、用户问题、AI回答）。下一次对话将在这个摘要的基础上继续。

**结果**: 即使原始的 "我叫Nemo" 消息被“删除”了，但它的信息被包含在了摘要里。因此，当用户再次提问时，模型通过读取摘要，依然能回答出用户的名字。

### 总结

这篇文档循序渐进地展示了如何为聊天机器人添加记忆：
1.  从最简单的**手动传递消息**开始，理解其基本原理。
2.  过渡到官方推荐的**使用 LangGraph 和 checkpointer 的自动记忆**，这是构建现代聊天应用的基础。
3.  最后介绍了两种高级技巧——**裁剪**和**摘要**——来优化和管理长对话的记忆，它们都是在 LangGraph 的 `call_model` 节点内对 `state` 进行处理来实现的。