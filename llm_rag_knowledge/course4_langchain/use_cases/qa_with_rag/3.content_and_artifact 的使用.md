```python
from langchain_core.tools import tool


@tool(response_format="content_and_artifact")
def retrieve(query: str):
    """Retrieve information related to a query."""
    retrieved_docs = vector_store.similarity_search(query, k=2)
    serialized = "\n\n".join(
        (f"Source: {doc.metadata}\nContent: {doc.page_content}")
        for doc in retrieved_docs
    )
    return serialized, retrieved_docs
```


---

### 核心思想：区分“给 LLM 看的内容”和“给系统用的数据”

在构建一个能使用工具的代理时，工具的返回值通常有两种用途：

1.  **给 LLM 看 (For the LLM to read)**: 这部分内容需要是**字符串**格式，简洁、易于理解。它会被插入到下一个提示（Prompt）中，作为 LLM 做出下一步决策或生成最终答案的依据。
2.  **给系统用 (For the system to use)**: 这部分可以是**任意 Python 对象**（如列表、字典、自定义类的实例等）。它不会被直接塞进提示里（因为会很冗长且格式混乱），但我们希望在工作流的后续步骤中能够访问到它，比如在最终结果中展示给用户，或者传递给另一个工具。

`@tool(response_format="content_and_artifact")` 这个装饰器就是为了完美地处理这种情况而设计的。

---

### 代码逐行详解

#### 1. `@tool(response_format="content_and_artifact")`

*   `@tool`: 这是一个 LangChain 装饰器，它能将一个普通的 Python 函数转换成一个 LLM 代理可以调用的“工具”。它会自动根据函数的签名（函数名、参数、类型提示、文档字符串）生成一个 JSON schema，让 LLM 知道这个工具是干什么的、叫什么名字、需要哪些参数。

*   `response_format="content_and_artifact"`: 这是最关键的部分。它告诉 `@tool` 装饰器，这个函数的返回值**不是一个单一的值，而是一个元组（tuple）**，这个元组包含两个部分：
    *   **第一部分是 `content`**: 这就是“给 LLM 看的”内容。**它必须是字符串**。LangChain Agent 会把这个字符串放入 `ToolMessage` 中，附加到对话历史里，以便 LLM 在下一轮“思考”时能够读到它。
    *   **第二部分是 `artifact`**: 这就是“给系统用的”数据。**它可以是任何类型的 Python 对象**。LangChain Agent 不会把它放进提示里，而是会把它**暂存起来**，通常是在状态（State）或内存中，供后续步骤使用。

#### 2. `def retrieve(query: str):`

这是一个标准的 Python 函数定义。`@tool` 会将它解析为：有一个名为 `retrieve` 的工具，它需要一个名为 `query` 的字符串参数。函数的文档字符串 `"""Retrieve information related to a query."""` 会成为 LLM 理解这个工具功能的描述。

#### 3. 函数体

```python
# 1. 在向量数据库中进行相似度搜索，返回最相关的 2 个文档对象
retrieved_docs = vector_store.similarity_search(query, k=2)

# 2. 将文档对象列表“序列化”成一个人类和 LLM 都易于阅读的字符串
serialized = "\n\n".join(
    (f"Source: {doc.metadata}\nContent: {doc.page_content}")
    for doc in retrieved_docs
)
```
*   `retrieved_docs` 是一个 `Document` 对象的列表，例如 `[Document(...), Document(...)]`。这是一个复杂的 Python 对象，不适合直接丢给 LLM。
*   `serialized` 是一个**格式化好的字符串**，例如：
    ```
    Source: {'source': 'doc_1.pdf'}
    Content: LangSmith helps with testing...

    Source: {'source': 'doc_2.pdf'}
    Content: You can run evaluations in LangSmith...
    ```
    这个字符串非常清晰，LLM 可以轻松地阅读和理解它，就像在阅读一篇文章一样。

#### 4. `return serialized, retrieved_docs`

这就是 `content_and_artifact` 模式的核心体现。

*   `serialized` (字符串): 这是元组的**第一个元素**，因此它被视为 **`content`**。它将被发送给 LLM 进行下一步的推理。
*   `retrieved_docs` (列表): 这是元组的**第二个元素**，因此它被视为 **`artifact`**。它将被系统保存下来，但不会污染 LLM 的上下文。

---

### 整个流程是如何工作的？(一个代理的例子)

想象一下这个工具在一个 LangGraph 工作流中被调用：

1.  **代理决策**: LLM 决定需要调用 `retrieve` 工具，并传入参数 `query="how to test apps?"`。

2.  **工具执行**: 你的 `retrieve` 函数被调用。它返回 `(serialized_string, list_of_documents)`。

3.  **LangChain 处理返回值**: LangChain 的代理执行器看到 `response_format` 是 `content_and_artifact`，于是它：
    *   **处理 `content`**: 创建一个 `ToolMessage`，其内容就是 `serialized_string`。这个消息会被加入到对话历史中。当再次调用 LLM 时，提示中会包含类似这样的内容：
        ```
        ...
        Human: how to test apps?
        AI: (决定调用 retrieve 工具)
        Tool: 
        Source: {'source': 'doc_1.pdf'}
        Content: LangSmith helps with testing...

        Source: {'source': 'doc_2.pdf'}
        Content: You can run evaluations in LangSmith...
        ```
        LLM 现在可以根据这个干净的文本内容来生成最终答案。

    *   **处理 `artifact`**: 将 `list_of_documents` 这个原始对象保存到图的状态（GraphState）的一个特定字段中，比如 `state['retrieved_artifacts']`。

4.  **最终输出**:
    *   LLM 根据提示中的 `content` 生成了最终答案，例如："You can test apps using LangSmith by creating datasets and running evaluations."
    *   你的应用程序此时可以从最终的状态中，同时获取 LLM 的**答案**和 `state['retrieved_artifacts']` 里的**原始文档对象**。
    *   这样，你就可以在界面上显示答案，并在答案下面附上可以点击的、包含完整元数据的来源链接，而这些来源链接就是用 `artifact` 构建的。

### 总结

`content_and_artifact` 是一种优雅的**数据隔离**机制。它解决了在代理工作流中的一个核心矛盾：

> LLM 需要**简洁的文本**来进行推理，而我们的系统和用户则需要**丰富的、结构化的原始数据**来进行后续处理和展示。

通过将工具的返回值一分为二，这个模式让 LLM 和系统各取所需，使得整个代理流程既高效又强大。

```python
from langchain_core.messages import SystemMessage
from langgraph.graph import END, MessagesState, StateGraph
from langgraph.prebuilt import ToolNode, tools_condition


class State(MessagesState):
    context: List[Document]


# Step 1: Generate an AIMessage that may include a tool-call to be sent.
def query_or_respond(state: State):
    """Generate tool call for retrieval or respond."""
    llm_with_tools = llm.bind_tools([retrieve])
    response = llm_with_tools.invoke(state["messages"])
    # MessagesState appends messages to state instead of overwriting
    return {"messages": [response]}


# Step 2: Execute the retrieval.
tools = ToolNode([retrieve])


# Step 3: Generate a response using the retrieved content.
def generate(state: MessagesState):
    """Generate answer."""
    # Get generated ToolMessages
    recent_tool_messages = []
    for message in reversed(state["messages"]):
        if message.type == "tool":
            recent_tool_messages.append(message)
        else:
            break
    tool_messages = recent_tool_messages[::-1]

    # Format into prompt
    docs_content = "\n\n".join(doc.content for doc in tool_messages)
    system_message_content = (
        "You are an assistant for question-answering tasks. "
        "Use the following pieces of retrieved context to answer "
        "the question. If you don't know the answer, say that you "
        "don't know. Use three sentences maximum and keep the "
        "answer concise."
        "\n\n"
        f"{docs_content}"
    )
    conversation_messages = [
        message
        for message in state["messages"]
        if message.type in ("human", "system")
        or (message.type == "ai" and not message.tool_calls)
    ]
    prompt = [SystemMessage(system_message_content)] + conversation_messages

    # Run
    response = llm.invoke(prompt)
    context = []
    for tool_message in tool_messages:
        context.extend(tool_message.artifact)
    return {"messages": [response], "context": context}
```

这是一段非常经典且完整的 LangGraph Agent 实现，它清晰地展示了“思考 -> 工具调用 -> 回答”的标准流程。我们来详细地解构这段代码的每一部分。

### 宏观视角：这个图在做什么？

这个 LangGraph 的目标是构建一个 RAG (Retrieval-Augmented Generation) 代理。它的行为逻辑是：

1.  接收用户的问题。
2.  **决策**：是直接回答，还是需要先调用 `retrieve` 工具查找资料？
3.  **工具调用**：如果需要，就执行 `retrieve` 工具。
4.  **生成答案**：利用检索到的信息，最终回答用户的问题。

这个图的巧妙之处在于，它通过 LLM 的工具调用能力，将“决策”和“生成”这两个步骤融合在了一起。

---

### 代码分步详解

#### 1. 定义状态 (`State`)

```python
from langgraph.graph import MessagesState
from typing import List
from langchain_core.documents import Document

class State(MessagesState):
    context: List[Document]
```

*   `MessagesState`: 这是一个 LangGraph 提供的便捷 `TypedDict`。它内置了一个 `messages` 字段（类型为 `List[BaseMessage]`），并且它的工作方式是**追加**而不是覆盖。当你从一个节点返回 `{"messages": [new_message]}` 时，`new_message` 会被添加到现有的 `messages` 列表中，这对于构建对话历史至关重要。
*   `context: List[Document]`: 我们在 `MessagesState` 的基础上**扩展**了我们自己的状态字段。`context` 字段将用于存储从 `retrieve` 工具的 `artifact` 部分返回的原始 `Document` 对象。这样，我们就可以在最后将这些来源返回给用户。

#### 2. 节点一：`query_or_respond` (决策与潜在的工具调用)

```python
def query_or_respond(state: State):
    """Generate tool call for retrieval or respond."""
    # 1. 将 `retrieve` 工具绑定到 LLM
    llm_with_tools = llm.bind_tools([retrieve])
    
    # 2. 调用 LLM，让它根据当前对话历史做决策
    response = llm_with_tools.invoke(state["messages"])
    
    # 3. 将 LLM 的响应（可能是工具调用，也可能是直接回答）追加到消息历史中
    return {"messages": [response]}
```

*   **第 1 步 (`llm.bind_tools`)**: 这是让 LLM “知道”并能“使用”工具的关键。它告诉 LLM：“你现在有一个名为 `retrieve` 的工具可以使用。如果你认为需要，你可以生成一个包含工具调用请求的特殊消息。”
*   **第 2 步 (`llm_with_tools.invoke`)**: 这是代理的“思考”步骤。LLM 会分析 `state["messages"]` 中的对话历史，然后做出判断：
    *   **情况 A：问题很简单或只是闲聊**。LLM 可能会直接生成一个普通的 `AIMessage`，内容就是答案。例如 `AIMessage(content="Hello there!")`。
    *   **情况 B：问题需要检索信息**。LLM 会生成一个包含**工具调用**的 `AIMessage`。这个消息看起来会像这样：`AIMessage(content="", tool_calls=[ToolCall(name='retrieve', args={'query': '...'})])`。它并没有直接回答，而是发出了一个“指令”。
*   **第 3 步 (`return {"messages": [response]}`)**: 将 LLM 的输出（无论是哪种情况）加入到状态中，供后续节点判断。

#### 3. 节点二：`tools` (工具执行节点)

```python
from langgraph.prebuilt import ToolNode

# 将 `retrieve` 函数包装成一个标准的 LangGraph 节点
tools = ToolNode([retrieve])
```

*   `ToolNode`: 这是 LangGraph 提供的一个预构建节点，专门用于执行工具。
*   **它的工作原理**: 它会自动检查传入的 `state["messages"]` 列表。如果列表中的最后一条消息是包含 `tool_calls` 的 `AIMessage`，它就会：
    1.  解析出工具名 (`retrieve`) 和参数。
    2.  调用对应的 Python 函数（我们定义的 `retrieve` 函数）。
    3.  将函数的返回值包装成一个 `ToolMessage`。
    4.  将这个 `ToolMessage` 追加到 `messages` 列表中。

*   **`ToolMessage` 的内容**: 因为我们的 `retrieve` 工具使用了 `response_format="content_and_artifact"`，所以生成的 `ToolMessage` 会非常特别：
    *   `message.content` 将会是 `retrieve` 函数返回的**字符串** (`serialized`)。
    *   `message.artifact` 将会是 `retrieve` 函数返回的**原始 Python 对象** (`retrieved_docs`)。

#### 4. 节点三：`generate` (利用工具结果生成最终答案)

这个节点只在工具被调用后才会执行。

```python
def generate(state: MessagesState):
    """Generate answer."""
    # 1. 从消息历史的末尾，提取出刚刚生成的 ToolMessage
    recent_tool_messages = []
    for message in reversed(state["messages"]):
        if message.type == "tool":
            recent_tool_messages.append(message)
        # 遇到非 tool 消息就停止，因为我们只关心最近一次的工具调用结果
        else:
            break
    tool_messages = recent_tool_messages[::-1]

    # 2. 将 ToolMessage 的 content 部分格式化成适合 LLM 阅读的上下文
    docs_content = "\n\n".join(doc.content for doc in tool_messages)
    system_message_content = (
        "You are an assistant... Use the following pieces of retrieved context..."
        f"{docs_content}"
    )
    
    # 3. 准备一个新的提示，包含原始问题和新的系统指令
    conversation_messages = [ ... ] # 过滤掉工具相关的消息，只保留对话本身
    prompt = [SystemMessage(system_message_content)] + conversation_messages

    # 4. 调用普通的 LLM（不带工具）来生成最终答案
    response = llm.invoke(prompt)
    
    # 5. 从 ToolMessage 的 artifact 部分提取原始文档对象，并存入 state['context']
    context = []
    for tool_message in tool_messages:
        context.extend(tool_message.artifact)
        
    # 6. 返回最终答案和来源文档
    return {"messages": [response], "context": context}
```

*   **第 1&2 步**: 这里我们从 `messages` 历史中**提取**出 `ToolNode` 刚刚添加的 `ToolMessage`，并将其 `content`（即我们精心格式化的字符串）准备好，作为生成最终答案的上下文。
*   **第 3&4 步**: 我们构建了一个**新的、临时的**提示。这个提示的核心是一个强大的 `SystemMessage`，它告诉 LLM：“根据我下面提供的这些检索到的信息 (`docs_content`) 来回答问题。” 然后调用 LLM 生成最终的 `AIMessage`。
*   **第 5 步**: **这步是关键**。我们再次遍历 `tool_messages`，但这次我们提取的是 `.artifact` 属性。这个属性里存着原始的、完整的 `Document` 对象列表。我们把它们收集起来，并存入我们自定义的 `State` 字段 `context` 中。
*   **第 6 步**: 更新状态，将最终的答案 `AIMessage` 追加到 `messages` 列表，并将提取出的来源文档列表存入 `context`。

### 5. 将所有节点连接成图 (未在代码段中显示，但逻辑如下)

```python
# (伪代码，展示图的构建逻辑)
from langgraph.prebuilt import tools_condition

graph_builder = StateGraph(State)

graph_builder.add_node("query_or_respond", query_or_respond)
graph_builder.add_node("tools", tools)
graph_builder.add_node("generate", generate)

graph_builder.set_entry_point("query_or_respond")

# 添加条件边：在 query_or_respond 之后，检查是否需要调用工具
graph_builder.add_conditional_edges(
    "query_or_respond",
    # tools_condition 是一个预构建的函数，它会检查最后一条消息是否有 tool_calls
    tools_condition,
    {
        # 如果需要工具，则路由到 "tools" 节点
        "tools": "tools",
        # 如果不需要，则流程结束
        "end": END
    }
)

# 从工具执行节点到最终生成节点
graph_builder.add_edge("tools", "generate")
# 从最终生成节点到结束
graph_builder.add_edge("generate", END)

graph = graph_builder.compile()
```

这个图的逻辑是：从 `query_or_respond` 开始，如果它决定调用工具，流程就走 `tools -> generate -> END`；如果它决定直接回答，流程就直接 `END`。

这套代码优雅地将代理的决策、工具使用和最终响应生成分离开来，同时通过 `content_and_artifact` 机制，确保了 LLM 的上下文保持干净，而系统又能保留丰富的源数据。