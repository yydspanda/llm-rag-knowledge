[参考链接](https://python.langchain.com/docs/how_to/#custom)

其他自定义方法，如：embeddin、retriever、data loader等，请参考上面链接。

我们来深入讲解一下如何使用 LangChain 创建一个自定义聊天模型类。

本文档的核心思想是，通过继承 LangChain 提供的基类，你可以将任何语言模型（无论是通过 API 调用、本地运行还是其他方式）封装成一个标准的 LangChain 聊天模型。这样做最大的好处是，你自定义的模型可以无缝地集成到 LangChain 的整个生态系统中，例如与 Agents、Chains 等组件一起使用，同时自动获得异步、流式传输等高级功能。

---

### 1. 核心概念：消息（Messages）

在 LangChain 中，聊天模型的输入和输出都是“消息”对象。这是一种标准化的格式，用于管理人与 AI 之间的对话历史。

主要的消息类型有：

*   `SystemMessage`: 系统消息，通常用于设定 AI 的角色或行为准则，放在对话的开头。
*   `HumanMessage`: 用户消息，代表用户输入的内容。
*   `AIMessage`: AI 消息，代表模型的回复。
*   `ToolMessage` / `FunctionMessage`: 工具或函数调用结果的消息，用于将外部工具的运行结果返回给模型。

此外，每种消息都有一个对应的“块”（Chunk）变体，如 `AIMessageChunk`，它们在流式传输（streaming）中使用，代表模型生成的部分响应。

---

### 2. 实现自定义聊天模型

要创建一个自定义聊天模型，你需要继承 `BaseChatModel` 类，并实现一些关键方法和属性。

#### 必须实现的方法/属性：

1.  `_generate`: 这是模型的核心逻辑所在。它接收一个消息列表作为输入，并返回一个完整的聊天结果 `ChatResult`。这是非流式调用的入口。
2.  `_llm_type` (属性): 一个返回字符串的属性，用于唯一标识你的模型类型，主要用于日志记录。

#### 可选实现的方法/属性：

*   `_stream`: 如果你的模型支持流式输出（即逐字或逐块返回结果），你需要实现这个方法。它应该返回一个生成器（Iterator），不断 `yield` `ChatGenerationChunk` 对象。
*   `_astream`: `_stream` 的异步版本。如果实现，可以获得更好的异步性能。如果不实现，LangChain 会自动在线程池中运行同步的 `_stream` 方法来实现异步流式传输。
*   `_agenerate`: `_generate` 的异步版本。
*   `_identifying_params` (属性): 返回一个字典，包含模型的关键参数。这些参数用于 LangChain 的回调系统（如 LangSmith），以便进行追踪和监控。

---

### 3. 代码实例：`ChatParrotLink` 模型

文档中提供了一个非常好的例子 `ChatParrotLink`，这个模型的功能很简单：**像鹦鹉学舌一样，重复输入消息的最后一条内容的开头 N 个字符**。我们来深入分析它的代码。

```python
from typing import Any, Dict, Iterator, List, Optional

from langchain_core.callbacks import CallbackManagerForLLMRun
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import (
    AIMessage,
    AIMessageChunk,
    BaseMessage,
)
from langchain_core.messages.ai import UsageMetadata
from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult
from pydantic import Field

class ChatParrotLink(BaseChatModel):
    """一个自定义聊天模型，它会回显输入的第一个 `parrot_buffer_length` 个字符。"""

    # 模型名称，可以通过别名 'model' 进行初始化
    model_name: str = Field(alias="model")
    # 需要回显的字符数
    parrot_buffer_length: int
    
    # --- 其他可选的模型参数 ---
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    timeout: Optional[int] = None
    stop: Optional[List[str]] = None
    max_retries: int = 2

    @property
    def _llm_type(self) -> str:
        """获取此聊天模型使用的语言模型类型。"""
        return "echoing-chat-model-advanced"

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """返回一个包含识别参数的字典，用于追踪。"""
        return {
            "model_name": self.model_name,
        }

    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """
        覆盖 _generate 方法来实现聊天模型的逻辑。
        这里可以是调用 API、本地模型或任何其他生成响应的实现。
        """
        # 1. 获取消息列表中的最后一条消息
        last_message = messages[-1]
        # 2. 根据 parrot_buffer_length 截取内容
        tokens = last_message.content[: self.parrot_buffer_length]

        # 3. (可选) 计算 token 使用量，用于监控
        ct_input_tokens = sum(len(message.content) for message in messages)
        ct_output_tokens = len(tokens)

        # 4. 将结果包装成 AIMessage
        message = AIMessage(
            content=tokens,
            additional_kwargs={},  # 用于传递额外信息
            response_metadata={  # 用于传递响应元数据
                "time_in_seconds": 3,
                "model_name": self.model_name,
            },
            usage_metadata={  # 包装 token 使用信息
                "input_tokens": ct_input_tokens,
                "output_tokens": ct_output_tokens,
                "total_tokens": ct_input_tokens + ct_output_tokens,
            },
        )
        
        # 5. 将 AIMessage 包装成 ChatGeneration
        generation = ChatGeneration(message=message)
        # 6. 最终返回 ChatResult
        return ChatResult(generations=[generation])

    def _stream(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> Iterator[ChatGenerationChunk]:
        """实现流式输出。"""
        last_message = messages[-1]
        tokens = str(last_message.content[: self.parrot_buffer_length])
        ct_input_tokens = sum(len(message.content) for message in messages)

        # 1. 逐字迭代要输出的内容
        for token in tokens:
            usage_metadata = UsageMetadata(
                {
                    "input_tokens": ct_input_tokens,
                    "output_tokens": 1,
                    "total_tokens": ct_input_tokens + 1,
                }
            )
            # 第一次循环后，输入token数设为0，因为后续的chunk是同一个请求的输出
            ct_input_tokens = 0
            # 2. 创建 AIMessageChunk
            chunk = ChatGenerationChunk(
                message=AIMessageChunk(content=token, usage_metadata=usage_metadata)
            )
            # 3. (可选) 调用回调函数，通知有新的 token 产生
            if run_manager:
                run_manager.on_llm_new_token(token, chunk=chunk)
            # 4. 使用 yield 返回数据块
            yield chunk

        # 5. (可选) 流式传输结束后，可以再 yield 一个空的 chunk 来传递最终的元数据
        chunk = ChatGenerationChunk(
            message=AIMessageChunk(
                content="",
                response_metadata={"time_in_sec": 3, "model_name": self.model_name},
            )
        )
        if run_manager:
            run_manager.on_llm_new_token("", chunk=chunk)
        yield chunk

```

#### 代码分析：

*   **初始化 (`__init__`)**: 这个类使用了 Pydantic 进行参数定义，比如 `model_name` 和 `parrot_buffer_length`。这些都是在创建模型实例时需要传入的参数。
*   **`_generate` 方法**: 这是最基础的实现。它完成了从接收请求到返回完整结果的整个流程。注意最后返回结果的层层包装：`str` -> `AIMessage` -> `ChatGeneration` -> `ChatResult`。这是 LangChain 内部约定的数据结构。
*   **`_stream` 方法**: 这个方法展示了如何实现流式响应。关键在于它是一个生成器函数，使用 `yield` 关键字来逐块返回数据。每一块 `ChatGenerationChunk` 都包含一小部分内容（在这个例子里是一个字符）。这种方式可以让用户在模型完全生成好答案之前，就看到部分结果，提升了交互体验。
*   **`_llm_type` 和 `_identifying_params`**: 这两个属性非常简单，但对于 LangChain 的可观察性（observability）和日志系统至关重要，建议总是实现它们。

---

### 4. 如何使用自定义模型

一旦你定义好了 `ChatParrotLink` 类，使用它就和使用任何 LangChain 官方集成的模型（如 `ChatOpenAI`）完全一样。

```python
# 1. 实例化模型
model = ChatParrotLink(parrot_buffer_length=3, model="my_custom_model")

# 2. 调用 invoke (非流式)
# 输入可以是字符串或消息列表
result = model.invoke("hello") 
print(result)
# 输出: AIMessage(content='hel', ...)

# 3. 调用 batch (批量处理)
results = model.batch(["hello", "goodbye"])
print(results)
# 输出: [AIMessage(content='hel', ...), AIMessage(content='goo', ...)]

# 4. 调用 stream (流式)
for chunk in model.stream("cat"):
    print(chunk.content, end="|")
# 输出: c|a|t||

# 5. 调用 astream (异步流式)
async for chunk in model.astream("cat"):
    print(chunk.content, end="|")
# 输出: c|a|t||
```

因为你的自定义模型遵循了 LangChain 的 `Runnable` 接口，所以 `invoke`, `batch`, `stream`, `astream` 这些标准方法都可以直接使用，无需任何额外代码。

### 总结

创建自定义聊天模型是 LangChain 框架强大扩展性的体现。通过遵循其接口规范（继承 `BaseChatModel` 并实现特定方法），你可以将任何文本生成逻辑融入其生态，无论是公司内部的私有模型、学术界的实验模型，还是某个尚未被官方支持的商业模型。这为你构建复杂的 LLM 应用提供了极大的灵活性。