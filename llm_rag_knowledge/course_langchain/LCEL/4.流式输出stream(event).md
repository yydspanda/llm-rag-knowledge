[参考链接](https://python.langchain.com/docs/how_to/streaming/)

好的，我们来仔细讲解这份关于**如何流式处理（Stream）Runnables** 的 LangChain 官方文档。

这份指南极其重要，因为它触及了构建**响应迅速、用户体验良好**的 AI 应用的核心技术。在 LLM 应用中，没有什么比让用户盯着一个加载圈长达数秒更糟糕的体验了。流式处理就是解决这个问题的关键。

文档将流式处理分为了两种核心方法，我们将逐一深入探讨。

---

### Part 1: 使用 `stream` / `astream` —— 流式传输“最终成品”

这是最基础、最常见的流式处理方法。

**核心思想**：
> `stream()` 和 `astream()` 的目标，是**以数据块（chunks）的形式，流式地返回链条的最终输出结果**。它就像让你提前看到正在打印的文档的每一行，而不是等整篇文档都打印完才给你。

**一个关键前提**:
这个方法要想“从头流到尾”，链条中的**每一个环节**都必须支持流式处理。也就是说，每个组件都必须能处理上一步传来的“数据块流”，并产生自己的“数据块流”传给下一步。

#### 1.1 从源头开始：流式处理 LLMs 和 Chat Models

LLM 是链条中最耗时的部分，因此也是流式处理最重要的起点。

*   **同步流 (`.stream()`)**:
    ```python
    for chunk in model.stream("what color is the sky?"):
        print(chunk.content, end="|", flush=True)
    ```
    *   **工作原理**: `model.stream()` 返回一个迭代器。每当 LLM 模型生成一小部分文本（一个或多个 token）时，这个迭代器就会 `yield` 一个数据块。
    *   **数据块的类型**: `AIMessageChunk`。这是一个特殊的对象，代表了 `AIMessage` 的一个片段。

*   **`AIMessageChunk` 的魔力：可加性**
    ```python
    chunks[0] + chunks[1] + chunks[2] ...
    ```
    *   `AIMessageChunk` 被设计成可以**直接相加**。将流式返回的所有 chunk 加在一起，你就可以随时重构出到目前为止已经生成的、完整的 `AIMessage`。这对于在前端实时更新显示内容非常方便。

#### 1.2 在链（Chains）中流式处理

LCEL 的美妙之处在于，流式处理是**自动继承**的。只要你的链条由支持流式处理的组件构成，整个链条就自然支持流式处理。

```python
prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")
parser = StrOutputParser()
chain = prompt | model | parser

async for chunk in chain.astream({"topic": "parrot"}):
    print(chunk, end="|", flush=True)
```
*   **工作流程**:
    1.  `prompt` 接收到输入，**立即**处理并将其输出传递给 `model`。（这一步非常快，几乎不产生可感知的流）
    2.  `model` 开始流式地生成 `AIMessageChunk`。
    3.  **关键**: `StrOutputParser` 是一个“**流式友好**”的解析器。它**不会**等待 `model` 生成完整的 `AIMessage`。相反，每当它接收到一个 `AIMessageChunk`，它就会**立即**处理这个小块（提取其 `.content`），并将其 `yield` 出来。
    4.  因此，我们最终得到的是一个个**字符串数据块（string chunks）**的流。

#### 1.3 处理更复杂的流：以 JSON 为例

流式处理字符串很简单，但流式处理 JSON 呢？一个不完整的 JSON 字符串（比如 `{"name": "Fra`）是无效的。

*   **智能解析器 (`JsonOutputParser`)**:
    ```python
    chain = model | JsonOutputParser()
    async for text in chain.astream(...):
        print(text, flush=True)
    ```
    *   `JsonOutputParser` 是一个“**流式智能**”的解析器。当它接收到不完整的 JSON 块时，它会**尝试“自动补全”**这个块，以生成一个在当前时间点**有效的** JSON 对象。
    *   **观察输出**: 你会看到它首先输出 `{'countries': []}`，然后是 `{'countries': [{'name': 'France'}]}`... 它在实时地构建和更新这个 JSON 对象，每一步都产生一个有效的中间状态。

#### 1.4 “打断”流式处理

**什么时候会中断？** 当链条中出现一个**不支持流式处理**的组件时。

```python
# _extract_country_names 是一个普通的 Python 函数，它需要完整的字典输入
chain = model | JsonOutputParser() | _extract_country_names

async for text in chain.astream(...):
    print(text, end="|", flush=True)
```
*   **结果**: 你会看到输出**不是**流式的，而是在所有内容都处理完毕后，**一次性**打印出最终结果 `['France', 'Spain', 'Japan']|`。
*   **原因**: `_extract_country_names` 函数不是一个生成器，它必须等待 `JsonOutputParser` 把它**完整**的、最终的 JSON 字典输出给它之后，才能开始工作。它就像一个“堤坝”，把上游的“数据流”全部拦截下来，直到水库满了才一次性放水。

#### 1.5 修复中断：使用生成器函数

我们可以把普通函数改写成**生成器（Generator）**函数（使用 `yield`），让它也变成流式友好的。

```python
async def _extract_country_names_streaming(input_stream):
    async for input in input_stream:
        # ... 逻辑 ...
        yield name
```
*   **工作原理**: 这个新函数现在可以处理一个**输入的流** (`input_stream`)。它在循环中接收 `JsonOutputParser` 实时生成的每一个中间状态的 JSON 字典，并从中 `yield` 出新的国家名。这样，“堤坝”就被改造成了“水电站”，数据流得以顺畅通过。

#### 1.6 遇到非流式组件怎么办？(如 Retriever)

有些组件，比如 `Retriever`，其本质就不是流式的（它需要完成搜索才能返回结果）。

```python
retrieval_chain = retriever | prompt | model | parser
```
*   **结果**: 流式输出**仍然有效**！
*   **工作原理**:
    1.  `retriever.stream()` 被调用。因为它不支持流，它会**阻塞**，直到完成搜索，然后**一次性**地 `yield` 出它所有的结果。
    2.  这个完整的结果被传递给 `prompt`，然后是 `model`。
    3.  **从 `model` 开始，流式处理恢复了！**
*   **结论**: LCEL 链非常健壮。即使中间有非流式组件，它也只会在那个点“暂停”一下，一旦该组件完成，后续的流式组件会**继续**进行流式输出。

---

### Part 2: 使用 `astream_events` —— 流式传输“过程中的一切”

这是 LangChain v0.2.0 引入的、一个更强大、更底层的流式 API。

**核心思想**:
> `astream_events` 的目标，是让你能够**实时地、细粒度地观察到链条中每一个组件的生命周期事件**。它不仅流式传输最终输出，还流式传输“谁开始了”、“谁正在处理什么”、“谁结束了”这些**元数据**。

这就像从“只看最终打印的文档”，升级到了可以观看印刷厂里**每一台机器的实时监控画面**。

#### 2.1 事件参考 (Event Reference)

文档提供了一个表格，列出了可能产生的事件类型，例如：
*   `on_chat_model_start`: 模型开始工作。
*   `on_chat_model_stream`: 模型产生了一个数据块。
*   `on_chat_model_end`: 模型工作结束。
*   `on_parser_start`, `on_parser_stream`, `on_parser_end`: 解析器的生命周期。
*   `on_chain_start`, `on_chain_end`: 整个链的生命周期。

每个事件都是一个包含 `event` 类型、`name` (组件名)、`run_id` 等信息的字典。

#### 2.2 实际应用

```python
async for event in chain.astream_events(...):
    kind = event["event"]
    if kind == "on_chat_model_stream":
        print(f"Chat model chunk: {event['data']['chunk'].content}")
    if kind == "on_parser_stream":
        print(f"Parser chunk: {event['data']['chunk']}")
```
*   **强大的可观测性**: 通过检查事件的 `kind` 和 `name`，你可以**精确地知道**当前这个数据块，是来自**哪个组件**的、处于**哪个阶段**的输出。
*   **即使流被“打断”也能工作**: 这是 `astream_events` 相对于 `astream` 的巨大优势！回到我们之前那个被 `_extract_country_names` 函数打断的例子，如果我们用 `astream_events`，我们**仍然**可以实时地接收到 `model` 和 `JsonOutputParser` 产生的流式事件。我们只是在最后一步看不到 `on_chain_stream` 事件，而会直接看到一个包含完整结果的 `on_chain_end` 事件。

#### 2.3 过滤事件

由于事件流信息量巨大，API 提供了过滤功能：
*   `include_names=["my_parser"]`: 只看名为 `my_parser` 的组件产生的事件。
*   `include_types=["chat_model"]`: 只看所有 `chat_model` 类型的组件产生的事件。
*   `include_tags=["my_chain"]`: 只看被标记为 `my_chain` 的组件（及其子组件）产生的事件。

#### 2.4 回调传播 (Propagating Callbacks)

这是一个高级但重要的细节。如果你在自定义的工具（`@tool`）或函数内部，又调用了另一个 `Runnable`（比如 `reverse_word.invoke(word)`），默认情况下，内部 `Runnable` 的事件**不会**被外部的 `astream_events` 捕捉到。

**正确做法**: 你必须手动将 `callbacks` 参数传递下去。
```python
@tool
def correct_tool(word: str, callbacks): # LangChain 会自动传入 callbacks
    return reverse_word.invoke(word, {"callbacks": callbacks})
```
*   **好消息**: 如果你是在 `RunnableLambda` 或 `@chain` 内部调用其他 `Runnable`，LangChain 会**自动为你处理**这个回调的传播。

### 总结

| | `.stream()` / `.astream()` | `.astream_events()` |
| :--- | :--- | :--- |
| **目标** | 流式传输**最终输出** | 流式传输**所有中间过程** |
| **输出内容** | 数据块 (e.g., `str`, `dict`) | 事件字典 (e.g., `{'event': 'on_model_stream', ...}`) |
| **优点** | 简单、直接，易于在前端展示 | **极其强大**的可观测性，能看到每个组件的实时状态 |
| **健壮性** | 如果链中有非流式组件，会**阻塞**并等待其完成 | 即使有非流式组件，**仍然能**流式传输其之前步骤的事件 |
| **适用场景** | 大部分需要在前端显示“打字机效果”的应用 | 需要**精细化调试**、构建**复杂监控面板**、或实时展示多步骤进度的应用 |