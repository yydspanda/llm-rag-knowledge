[参考链接](https://python.langchain.com/docs/how_to/dynamic_chain/)

好的，我们来仔细讲解这份关于**如何创建动态（自我构建）链**的 LangChain 官方文档。

这份指南介绍的是 LCEL 中一个极其强大、极其灵活，同时也可能是最令人“脑洞大开”的特性。它将上一份指南中的“路由”思想，提升到了一个新的维度。

---

### 核心思想：从“选择”一条路，到“构建”一条路

在上一份关于路由的指南中，我们的 `route` 函数**返回**了一个**已经存在**的 `Runnable`（比如 `anthropic_chain` 或 `langchain_chain`）。这就像一个铁路调度员，在几条**固定的、预先铺设好的轨道**之间，选择一条让火车通过。

而这份指南介绍的“动态构建”，则更进一步：
> **我们的函数不再只是从固定的选项中选择一条路，而是可以根据输入，动态地、即时地“凭空”构建出接下来要走的路。**

**核心比喻：从“铁路调度员”到“魔法工程师”**
*   **铁路调度员 (旧的路由)**：看着地图（`if/else`），扳动道岔，让火车（数据流）开上**已有的**轨道 A 或轨道 B。
*   **魔法工程师 (动态构建)**：看着眼前的地形（输入数据），**瞬间**在面前**创造并铺设**出一条全新的轨道，然后让火车开上去。这条新轨道可能是一段简单的直线，也可能是一段复杂的回旋。

这个“魔法”的实现，依赖于 `RunnableLambda`（或 `@chain` 装饰器）的一个关键特性：
> **如果一个 `RunnableLambda` 返回的不是一个最终的值（如字符串、字典），而是另一个 `Runnable` 对象，那么 LCEL 运行时会自动地、接续地执行那个被返回的 `Runnable`。**

---

### 示例详解：一个“按需”的 RAG 链

这个例子构建了一个非常实用的、智能的 RAG 链。它的智能之处在于，它能**根据对话历史是否存在，来动态地决定是否需要增加一个“问题改写”的步骤**。

#### 1. 准备组件

*   **`contextualize_question` (问题改写链)**:
    *   这是一个标准的 LCEL 链，它的任务是：结合聊天历史，将用户的后续问题（比如 "what about egypt"）改写成一个独立的、完整的、无歧义的问题（"what is the population of Egypt?"）。
*   **`qa_prompt` (问答 Prompt)**:
    *   一个标准的 RAG Prompt，需要 `{context}` 和 `{question}`。
*   **`fake_retriever` (伪检索器)**:
    *   一个用 `@chain` 装饰的函数，模拟检索过程，总是返回关于埃及人口的固定文本。

#### 2. “魔法工程师”的实现 (`contextualize_if_needed`)

这是整个系统的核心和灵魂。

```python
from langchain_core.runnables import Runnable, RunnablePassthrough, chain
from operator import itemgetter

@chain
def contextualize_if_needed(input_: dict) -> Runnable:
    # 检查输入中是否存在 'chat_history'
    if input_.get("chat_history"):
        # 如果有历史，就“构建”并返回“问题改写”这条轨道
        # 注意：这里返回的是一个 Runnable 对象！
        return contextualize_question
    else:
        # 如果没有历史，就“构建”并返回一条极简的“直通”轨道
        return RunnablePassthrough() | itemgetter("question")
```
*   **函数签名 `-> Runnable`**: 我们明确地注解，这个函数的返回值是一个 `Runnable`。这不仅是好的编程习惯，也能帮助我们理解其意图。
*   **`if input_.get("chat_history")`**: 这是“魔法工程师”的**决策逻辑**。它检查输入的地形，看看是否有“聊天历史”这座山。
*   **`return contextualize_question`**:
    *   如果存在聊天历史，工程师说：“好的，路况复杂，我们需要先走一段‘问题改写’的路。”
    *   它**返回**了我们之前定义好的 `contextualize_question` 这个**完整的 `Runnable` 链**。
*   **`return RunnablePassthrough() | itemgetter("question")`**:
    *   如果不存在聊天历史（说明这是第一轮对话），工程师说：“路况很简单，我们直接走直线就行。”
    *   它**动态地构建并返回**了一个新的、极其简单的 `Runnable`。这个 `Runnable` 的作用是：接收一个字典输入，然后从中提取出 `"question"` 字段的值。

#### 3. 组装完整的主链

```python
full_chain = (
    RunnablePassthrough.assign(question=contextualize_if_needed).assign(
        context=fake_retriever
    )
    | qa_prompt
    | llm
    | StrOutputParser()
)
```
*   **`RunnablePassthrough.assign(question=contextualize_if_needed)`**:
    *   这是最关键的一步。我们使用 `.assign` 来**覆盖或创建** `question` 这个字段。
    *   它的值，将由 `contextualize_if_needed` 这个“魔法工程师”**动态地决定并构建**的那个 `Runnable` 来计算。

---

### 追踪两次不同的调用，见证“魔法”的发生

#### 场景 A: 有聊天历史 (需要改写)

`full_chain.invoke({"question": "what about egypt", "chat_history": [...]})`

1.  **`.assign(question=...)` 开始执行**:
    *   `contextualize_if_needed` 被调用，输入是 `{"question": ..., "chat_history": ...}`。
    *   `if` 条件为 `True`。
    *   `contextualize_if_needed` **返回**了 `contextualize_question` 这个 `Runnable`。
2.  **LCEL 运行时接管**:
    *   LCEL 看到返回值是一个 `Runnable`，于是它**自动地**用**原始输入** `{"question": ..., "chat_history": ...}` 去 `.invoke()` 这个被返回的 `contextualize_question` 链。
3.  **问题被改写**:
    *   `contextualize_question` 链执行，输出改写后的问题（一个字符串）: `"What is the population of Egypt?"`。
4.  **`.assign` 完成工作**:
    *   这个改写后的字符串，成为了 `question` 字段的新值。
    *   此时的数据流状态变为：`{"question": "What is the population of Egypt?", "chat_history": [...]}`。
5.  **后续流程**:
    *   `.assign(context=fake_retriever)` 执行，增加了 `context` 字段。
    *   最终，一个包含了**改写后的 `question`** 和 `context` 的完美字典，被送入了 `qa_prompt`。

#### 场景 B: 没有聊天历史 (无需改写)

`full_chain.invoke({"question": "what's the population of indonesia"})`

1.  **`.assign(question=...)` 开始执行**:
    *   `contextualize_if_needed` 被调用，输入是 `{"question": "..."}`。
    *   `if` 条件为 `False`。
    *   `contextualize_if_needed` **返回**了 `RunnablePassthrough() | itemgetter("question")` 这个 `Runnable`。
2.  **LCEL 运行时接管**:
    *   LCEL **自动地**用**原始输入** `{"question": "..."}` 去 `.invoke()` 这个被返回的 `Runnable`。
3.  **问题被“直通”**:
    *   `RunnablePassthrough()` 接收到字典，原样输出。
    *   `itemgetter("question")` 接收到字典，提取出 `"question"` 的值。
    *   最终输出是原始问题字符串: `"what's the population of indonesia"`。
4.  **`.assign` 完成工作**:
    *   这个**未经改写的**字符串，成为了 `question` 字段的值。
    *   此时的数据流状态变为：`{"question": "what's the population of indonesia"}`。
5.  **后续流程**:
    *   `.assign(context=...)` 执行，下游 `qa_prompt` 接收到**原始的 `question`** 和 `context`。

### 总结：这有什么用？

这种“动态构建”或“自我构建”的模式，是构建**自适应（Adaptive）** AI 系统的终极技巧。

1.  **条件逻辑的终极形态**: 它比 `RunnableBranch` 或简单的函数路由更强大，因为它不仅能**选择**路径，还能**创造**路径。
2.  **按需加载 (On-demand Loading)**: 你可以构建一个 Agent，它只有在需要时，才去动态地加载和构建一个非常复杂、非常消耗资源的子链，而不是在一开始就全部定义好。
3.  **高度模块化和可维护性**: `contextualize_if_needed` 这个组件的职责非常单一和清晰——“决定是否需要改写，并提供相应的改写或直通逻辑”。你可以独立地测试和优化它，而无需关心主链条的其他部分。

掌握了这个技巧，你就真正从一个“链条的使用者”，变成了一个“**链条的创造者**”，能够构建出能够根据上下文，**动态地重构自身处理流程**的、真正智能的 AI 应用。