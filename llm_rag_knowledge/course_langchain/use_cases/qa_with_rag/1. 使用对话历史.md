我们来深入讲解一下 LangChain 中**如何结合聊天记录进行问答（QA with Chat History）**。这在构建一个能进行多轮对话的问答机器人时至关重要。

本文档的核心思想是解决一个常见问题：当用户提出一个后续问题时（例如 "它是什么时候发布的？"），模型需要理解问题中的 "它" 指的是上一轮对话中提到的主题。如果直接将 "它是什么时候发布的？" 拿去检索，系统很可能找不到任何相关的文档。

---

### 1. 问题所在：无状态的问答

一个基础的问答（QA）链通常是这样工作的：

1.  接收用户的问题。
2.  将问题转化为向量，在向量数据库中检索最相关的文档片段。
3.  将问题和检索到的文档片段（作为上下文）一起提交给语言模型。
4.  语言模型根据上下文来回答问题。

这个流程是**无状态**的。每一轮问答都是独立的，它不记得之前的对话内容。

*   **第一轮对话：**
    *   用户："LangChain 是什么？"
    *   系统：（检索关于 LangChain 的文档）... "LangChain 是一个开发 LLM 应用的框架..."
*   **第二轮对话：**
    *   用户："它有什么特点？"
    *   系统：（直接检索 "它有什么特点？"）-> **失败！** 系统不知道 "它" 指的是 LangChain，因此无法检索到有用的文档。

---

### 2. 解决方案：历史感知检索器 (History-Aware Retriever)

为了解决这个问题，我们需要让检索器能够“感知”到聊天记录。具体方法分为两步：

1.  **问题重构 (Question Rewriting)**：在进行检索之前，我们先利用语言模型，将**聊天记录**和**用户的后续问题**结合起来，生成一个**独立的、无需上下文就能理解的**新问题。
    *   **输入**:
        *   聊天记录: `[Human: "LangChain 是什么？", AI: "LangChain 是一个...框架..."]`
        *   后续问题: `"它有什么特点？"`
    *   **中间步骤 (LLM 调用)**:
        *   提示 (Prompt): "根据下面的聊天记录和后续问题，将后续问题改写成一个独立的问题。"
        *   LLM 输出 (独立问题): `"LangChain 有什么特点？"`
2.  **执行检索**: 使用这个重构后的独立问题（"LangChain 有什么特点？"）去向量数据库中检索相关的文档。

通过这个中间步骤，我们的检索器就变得“智能”了，它总能拿到一个完整的问题去查找资料。LangChain 将这个过程封装成了一个**历史感知检索器 (History-Aware Retriever)**。

---

### 3. LangChain 代码实现深入解析

文档中的代码清晰地展示了如何构建这个流程。我们一步步来分析。

#### 第一步：设置基础组件

首先，需要准备一个语言模型（LLM）和一个检索器（Retriever）。检索器是基于一个向量数据库建立的，这里用一个简单的内存向量库作为例子。

```python
# 导入所需库
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.messages import HumanMessage, AIMessage

# 初始化 LLM 和 嵌入模型
llm = ChatOpenAI()
embeddings = OpenAIEmbeddings()

# 创建一个简单的向量存储作为检索器
docs = ... # 省略示例文档
vectorstore = FAISS.from_documents(docs, embedding=embeddings)
retriever = vectorstore.as_retriever()
```

#### 第二步：创建历史感知检索器

这是实现对话记忆的关键。我们需要一个特殊的提示（Prompt）来指导 LLM 如何重构问题。

```python
# 1. 定义问题重构的提示
# 这个提示告诉模型它的任务是：根据聊天记录，把新问题（input）变成一个独立的问题
prompt = ChatPromptTemplate.from_messages([
    ("placeholder", "{chat_history}"),
    ("user", "{input}"),
    ("user", "Given the above conversation, generate a search query to look up in order to get information relevant to the conversation")
])

# 2. 使用 LangChain 的辅助函数创建历史感知检索器链
# 这个链接收 chat_history 和 input, 输出是检索到的文档
retriever_chain = create_history_aware_retriever(llm, retriever, prompt)
```

*   `ChatPromptTemplate.from_messages`: 我们用它来构建一个对话式的提示。
*   `("placeholder", "{chat_history}")`: 这是一个占位符，LangChain 会自动将 `Messages` 对象列表（如 `[HumanMessage(...), AIMessage(...)]`）填充到这里。
*   `create_history_aware_retriever`: 这个高级辅助函数将 LLM、检索器和问题重构提示组合在一起，创建出一个新的链。这个链的内部逻辑就是我们前面分析的**“问题重构 -> 检索”**。

我们可以测试一下这个 `retriever_chain`：

```python
chat_history = [HumanMessage(content="Can LangSmith help test my LLM applications?"), AIMessage(content="Yes!")]
# 调用这个链，它会返回与 "testing LLM applications with LangSmith" 相关的文档
retriever_chain.invoke({
    "chat_history": chat_history,
    "input": "Tell me how"
})
```
在这个例子中，`retriever_chain` 会先把 `"Tell me how"` 改写成类似 `"How can LangSmith help test my LLM applications?"`，然后再用这个新问题去检索。

#### 第三步：创建最终的问答链

现在我们有了一个能智能检索文档的链，但我们还需要最后一步：**将检索到的文档和原始问题结合起来，生成最终答案**。

```python
# 1. 定义生成最终答案的提示
# 这个提示接收上下文（context，即检索到的文档）和问题（input），并要求模型回答问题
prompt = ChatPromptTemplate.from_messages([
    ("system", "Answer the user's questions based on the below context:\n\n{context}"),
    ("placeholder", "{chat_history}"),
    ("user", "{input}"),
])

# 2. 创建一个“文档处理链”
# create_stuff_documents_chain 会将所有文档内容“塞入”到上面提示的 {context} 变量中
document_chain = create_stuff_documents_chain(llm, prompt)

# 3. 将历史感知检索器链和文档处理链组合成最终的检索链
# 这是整个流程的核心：先用 retriever_chain 获取文档，再用 document_chain 生成答案
retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)
```

*   `create_stuff_documents_chain`: 这是一个常用的辅助函数，它的作用是接收一系列文档，将它们的内容合并成一个字符串，然后放入提示的 `{context}` 变量中。
*   `create_retrieval_chain`: 这是另一个高级辅助函数，它将前面两步完美地结合起来。它的内部工作流程是：
    1.  接收 `chat_history` 和 `input`。
    2.  将它们传递给第一个参数 `retriever_chain`（我们的历史感知检索器）。
    3.  `retriever_chain` 重构问题并返回相关文档。
    4.  然后，它将 `input`, `chat_history` 和上一步返回的文档（作为 `context`）一起传递给第二个参数 `document_chain`。
    5.  `document_chain` 调用 LLM 生成最终答案。
    6.  返回一个包含 `answer`、`context` 等信息的字典。

#### 第四步：进行多轮对话测试

现在，我们可以像一个真正的聊天机器人一样与 `retrieval_chain` 交互了。

```python
chat_history = []

# 第一轮
first_input = "Can LangSmith help test my LLM applications?"
response = retrieval_chain.invoke({
    "chat_history": chat_history,
    "input": first_input
})
print(response["answer"])
# 输出: Yes, LangSmith can help test your LLM applications...

# 更新聊天记录
chat_history.extend([HumanMessage(content=first_input), AIMessage(content=response["answer"])])

# 第二轮
second_input = "Tell me how"
response = retrieval_chain.invoke({
    "chat_history": chat_history,
    "input": second_input
})
print(response["answer"])
# 输出: LangSmith provides several tools to help you test your LLM applications...
```

可以看到，在第二轮对话中，我们只输入了 "Tell me how"，但系统成功理解了我们的意图，并根据之前关于 LangSmith 的上下文给出了详细的回答。这就是整个流程的威力所在。

### 总结

构建一个带聊天记录的问答系统，关键在于**在检索前先重构问题**。LangChain 通过 `create_history_aware_retriever` 和 `create_retrieval_chain` 这两个高级辅助函数，将这个相当复杂的逻辑流程（LLM 调用 -> 检索 -> 另一次 LLM 调用）封装得非常简洁和优雅，让开发者可以轻松地为自己的问答应用增加上下文记忆功能。