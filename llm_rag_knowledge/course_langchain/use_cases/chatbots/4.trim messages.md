
### 核心问题：为什么需要裁剪消息 (Trimming Messages)？

所有的大语言模型（LLM）都有一个“上下文窗口（Context Window）”的限制。你可以把它想象成模型的**短期记忆容量**。这个容量不是无限的，它有一个最大的 Token（可以理解为单词或字符块）数量限制。

当你构建一个聊天机器人时，为了让对话能够连续，你会把之前的对话历史不断地累加起来，连同用户的新问题一起发给模型。如果对话一直持续下去，这个历史记录会越来越长，迟早会发生两件事：

1.  **超出限制**：消息总长度超过了模型的上下文窗口，API 会直接报错。
2.  **成本和效率**：即使没有超出限制，发送非常长的历史记录也会消耗更多的 Token，导致费用增加，并且可能因为信息过多而降低模型的响应速度和准确性。

**`trim_messages`** 就是 LangChain 提供的、用来优雅地解决这个问题的“剪刀”。

---

### `trim_messages` 的核心功能与挑战

`trim_messages` 的主要功能是：**将一个消息列表，根据指定的规则，缩减到目标大小（按 Token 数或消息数）**。

但这不仅仅是简单地从开头或结尾删除消息。一个被裁剪后的聊天记录，如果想让模型能正确理解，必须是**有效的（valid）**。文档中提到了一个有效的聊天记录通常需要满足的规则：

1.  **开头有效**：必须以 `HumanMessage` 开头，或者以 `SystemMessage` -> `HumanMessage` 的顺序开头。不能以 `AIMessage` 开头。
2.  **结尾有效**：通常期望以 `HumanMessage` (用户最新提问) 或 `ToolMessage` (工具返回结果) 结尾。
3.  **结构有效**：`ToolMessage` 必须跟在一个请求调用工具的 `AIMessage` 之后。
4.  **保留系统指令**：`SystemMessage` 通常包含了对模型的关键指令（比如“你是一个乐于助人的助手”），我们希望在裁剪后它能被保留下来。

`trim_messages` 的强大之处在于，它的各种参数就是为了帮助我们轻松地遵守这些规则。

---

### 1. 按 Token 数量裁剪（最常用）

这是最精确的裁剪方式，旨在确保消息总长度不超过模型的具体 Token 限制。

**代码讲解**:
```python
from langchain_core.messages import trim_messages, SystemMessage, HumanMessage, AIMessage
from langchain_core.messages.utils import count_tokens_approximately

messages = [...] # 一个很长的消息列表

trimmed = trim_messages(
    messages,
    strategy="last",
    token_counter=count_tokens_approximately,
    max_tokens=45,
    start_on="human",
    end_on=("human", "tool"),
    include_system=True,
    allow_partial=False,
)
```

让我们逐一分解这些参数，理解它们如何协同工作：

*   `messages`: 你要裁剪的原始消息列表。
*   `strategy="last"`: **裁剪策略**。`"last"` 表示**保留最新的消息，丢弃最旧的消息**。这是最常用的策略，因为它保留了最近的对话上下文。与之相对的是 `"first"`，保留最开始的消息。
*   `token_counter`: **令牌计数器**。这是一个函数，用来计算消息列表的 Token 总数。
    *   `count_tokens_approximately` 是一个内置的、快速但**不精确**的估算器。
    *   后面会讲到，这里可以传入一个模型实例（如 `ChatOpenAI(...)`）来进行精确计算。
*   `max_tokens=45`: **目标大小**。裁剪后的消息列表，其 Token 总数将小于或等于 45。
*   `include_system=True`: **保留系统消息**。这个参数会告诉裁剪器：“如果原始列表的第一条消息是 `SystemMessage`，请无条件地将它保留在裁剪结果的开头。” 这非常重要，可以确保模型的角色和指令不会丢失。
*   `start_on="human"`: **确保开头有效**。在保留了系统消息（如果 `include_system=True`）之后，这个参数会确保接下来的第一条消息是 `HumanMessage`，从而满足我们前面提到的“有效性规则1”。
*   `end_on=("human", "tool")`: **确保结尾有效**。这个参数在这里文档中提到，但在此示例中，由于`strategy="last"`，它通常会自动满足，因为最新的消息就是`HumanMessage`。
*   `allow_partial=False`: **不允许部分消息**。`False` 表示裁剪器只能丢弃整条消息，不能把一条消息的内容切开。如果设为 `True`，当 Token 差一点点就要超标时，它会把某条消息的内容截断一部分以满足 `max_tokens` 的限制。

**执行结果分析**:
原始 `messages` 列表很长。`trim_messages` 会从**后往前**开始保留消息，同时用 `token_counter` 累加 Token 数，直到总数接近但不超过 `max_tokens=45`。因为 `include_system=True`，它还会把第一条 `SystemMessage` 也加进来。最终，它发现保留 `SystemMessage` 和**最后一条** `HumanMessage` 是唯一满足条件的组合。

---

### 2. 按消息数量裁剪（更简单）

有时候我们不需要精确的 Token 控制，只想保留比如“最近5条消息”。

**代码讲解**:
```python
trim_messages(
    messages,
    strategy="last",
    token_counter=len, # <- 关键变化
    max_tokens=5,      # <- 现在代表消息数量
    include_system=True,
    ...
)
```
*   **关键技巧**: `token_counter=len`。我们把计数器换成了 Python 内置的 `len` 函数。当 `trim_messages` 调用 `len(message_list)` 时，它得到的是列表的长度，即消息的数量。
*   通过这个巧妙的替换，`max_tokens=5` 的含义就从“最多5个 Token”变成了“**最多5条消息**”。
*   其他参数 (`strategy`, `include_system` 等) 的作用和之前完全一样。

---

### 3. 在 LangChain 表达式语言 (LCEL) 中使用（最实用）

在真实的 LCEL 应用中，我们不会手动去调用 `trim_messages`。我们会把它作为一个**组件**插入到链（Chain）中。

**代码讲解**:
```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o")

# 1. 创建一个“裁剪器”组件
#    注意：这里我们没有传入 messages 参数！
trimmer = trim_messages(
    max_tokens=45,
    strategy="last",
    token_counter=llm, # <- 使用模型本身作为精确的计数器
    include_system=True,
    start_on="human",
)

# 2. 将裁剪器放入链中
chain = trimmer | llm

# 3. 调用链
chain.invoke(messages) # 在这里传入完整的、未裁剪的消息
```
*   **声明式用法**: 当我们调用 `trim_messages(...)` 却**不传入 `messages` 参数**时，它不会立即执行。相反，它会返回一个 `Runnable` 对象（即一个 LCEL 组件），这个组件“知道”如何进行裁剪，并**等待着输入**。
*   `token_counter=llm`: 这是一个非常好的实践。我们直接把 `ChatOpenAI` 模型实例作为计数器。这样，裁剪时使用的 Token 计算方法将与模型最终接收消息时**完全一致**，实现了最精确的控制。
*   **数据流**: 当 `chain.invoke(messages)` 被调用时：
    1.  完整的、可能超长的 `messages` 列表首先被传递给 `trimmer`。
    2.  `trimmer` 对其进行裁剪，输出一个符合 `max_tokens` 限制的、更短的 `trimmed_messages` 列表。
    3.  这个 `trimmed_messages` 列表接着被传递给 `llm`。
    4.  `llm` 在一个永远不会超长的、安全的消息列表上进行处理。

---

### 4. 与聊天记录 `ChatMessageHistory` 结合

这可以说是 `trim_messages` 最重要的应用场景。

```python
from langchain_core.runnables.history import RunnableWithMessageHistory

# ... 设置 chat_history 和 dummy_get_session_history ...

# 创建一个和上面一样的 trimmer
trimmer = trim_messages(...)

# 创建一个基础链
chain = trimmer | llm

# 用 RunnableWithMessageHistory 包装基础链
chain_with_history = RunnableWithMessageHistory(
    chain,
    dummy_get_session_history,
    ...
)

chain_with_history.invoke(
    [HumanMessage("what do you call a speechless parrot")],
    config={"configurable": {"session_id": "1"}},
)```
*   **工作流程**:
    1.  `chain_with_history.invoke` 被调用。
    2.  `RunnableWithMessageHistory` 首先会根据 `session_id` 调用 `dummy_get_session_history`，**加载出完整的、可能非常长的历史记录**。
    3.  它将加载出的历史记录和用户的新问题合并成一个超长的 `messages` 列表。
    4.  这个超长的列表被传递给 `chain`。
    5.  `chain` 的第一步是 `trimmer`，它立刻将这个超长的列表**裁剪**成一个安全、简短的列表。
    6.  裁剪后的列表被传递给 `llm`。

**最终效果**: 无论 `chat_history` 里存了多少条消息（10条或10000条），`trimmer` 都会像一个“门卫”一样，确保最终进入 LLM 的只有最近的、符合 `max_tokens` 限制的消息，从而完美地解决了上下文窗口溢出的问题。