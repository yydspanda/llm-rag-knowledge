### 核心概念：为什么需要检索？

一个普通的聊天机器人（LLM）只能回答它在训练时学到的知识。这有两个主要问题：
1.  **知识过时**：它不知道训练日期之后发生的事情。
2.  **私有数据**：它不知道你的公司内部文档、个人笔记等私有信息。

**检索**就是解决这个问题的技术。它的核心思想是：当用户提问时，我们不直接把问题扔给 LLM，而是先去一个**外部知识库**（比如你的文档、网站、数据库）中**搜索**相关的资料，然后将“**用户的问题 + 搜索到的相关资料**”一起作为上下文，提交给 LLM，让它基于这些新鲜、准确的资料来回答问题。

这篇文档将一步步教我们如何构建这样一个具备检索能力的聊天机器人。

---

### 1. 创建一个检索器 (Creating a retriever)

这是整个系统的基础：一个能够根据查询从知识库中取回相关文档的工具。

**步骤 1.1: 加载数据 (Load)**
首先，我们需要一个知识库。文档以 LangSmith 的在线文档为例。

```python
from langchain_community.document_loaders import WebBaseLoader

loader = WebBaseLoader("https://docs.smith.langchain.com/overview")
data = loader.load()
```
*   **讲解**: `WebBaseLoader` 是一个文档加载器，它能像浏览器一样访问一个网址，并抓取其网页内容作为文本数据。`data` 现在是一个包含了该网页所有文本的列表。

**步骤 1.2: 切分文档 (Split)**
LLM 的上下文窗口是有限的，我们不能把一整篇长文档都塞进去。因此，需要将它切分成更小的、语义完整的块（chunks）。

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)
all_splits = text_splitter.split_documents(data)
```
*   **讲解**: `RecursiveCharacterTextSplitter` 是一个智能的文本切分器。它会尝试按段落、句子等方式来切分，确保切分后的块尽可能保持语义完整。
    *   `chunk_size=500`: 每个块的大小约为500个字符。
    *   `chunk_overlap=0`: 块与块之间没有重叠。

**步骤 1.3: 嵌入并存入向量数据库 (Embed and Store)**
计算机不理解文本的含义，但能理解数字。我们需要将文本块转换成“词向量”（Embeddings），这是一种能够代表文本语义的数学表示。然后将这些向量存入一个专门的数据库——向量数据库，以便进行快速的相似度搜索。

```python
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())
```
*   **讲解**:
    *   `OpenAIEmbeddings()`: 这是一个嵌入模型，负责将文本块转换成向量。
    *   `Chroma`: 这是一个内存中的向量数据库，非常适合快速原型开发。
    *   `Chroma.from_documents(...)`: 这行代码完成了两件事：1. 调用 `OpenAIEmbeddings` 将 `all_splits` 中的每个文档块转换成向量；2. 将文档原文和对应的向量一起存入 `Chroma` 数据库中。

**步骤 1.4: 创建检索器 (Retrieve)**
最后，我们将这个向量数据库包装成一个“检索器”对象。

```python
retriever = vectorstore.as_retriever(k=4)
```
*   **讲解**: `as_retriever()` 方法创建了一个检索器。
    *   `k=4`: 表示每次进行检索时，返回与查询最相似的 **4个** 文档块。

至此，我们已经有了一个可以工作的 `retriever`。当你调用 `retriever.invoke("一个问题")`，它就会去 Chroma 数据库里找出与这个问题最相关的4个文本块并返回。

---

### 2. 文档链 (Document chains)

我们有了检索器，但 LLM 还不知道怎么使用它检索出的文档。我们需要创建一个“链”（Chain），告诉 LLM 如何基于我们提供的上下文（即检索出的文档）来回答问题。

```python
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# 1. 定义系统提示模板
SYSTEM_TEMPLATE = """
Answer the user's questions based on the below context. 
If the context doesn't contain any relevant information to the question, don't make something up and just say "I don't know":

<context>
{context}
</context>
"""

# 2. 创建完整的问答提示
question_answering_prompt = ChatPromptTemplate.from_messages([
    ("system", SYSTEM_TEMPLATE),
    MessagesPlaceholder(variable_name="messages"),
])

# 3. 创建文档链
document_chain = create_stuff_documents_chain(chat, question_answering_prompt)
```
*   **讲解**:
    *   `SYSTEM_TEMPLATE`: 这是给 LLM 的核心指令。`{context}` 是一个占位符，我们稍后会把检索到的文档**填充（stuff）**到这里。
    *   `create_stuff_documents_chain`: 这是一个辅助函数，它创建了一个链，这个链的输入需要包含一个 `context` (文档列表) 和 `messages` (对话历史)。它会自动将 `context` 中的所有文档格式化后填入提示模板的 `{context}` 位置。
    *   `document_chain` 现在是一个可以接受上下文和问题，并生成答案的组件。

---

### 3. 检索链 (Retrieval chains)

现在，我们将**第1步的 `retriever`** 和 **第2步的 `document_chain`** 串联起来，形成一个完整的、自动化的检索问答链。

```python
from langchain_core.runnables import RunnablePassthrough

def parse_retriever_input(params: Dict):
    return params["messages"][-1].content

retrieval_chain = RunnablePassthrough.assign(
    context=parse_retriever_input | retriever,
).assign(
    answer=document_chain,
)
```
*   **讲解 (这部分使用了 LangChain 表达式语言 LCEL，非常关键)**:
    1.  `RunnablePassthrough.assign(...)`: 这是一个 LCEL 的常见用法，用于并行处理和丰富数据流。
    2.  `context=parse_retriever_input | retriever`:
        *   `parse_retriever_input`: 我们定义了一个小函数，从输入的 `messages` 列表中提取出最后一个消息（即用户最新的问题）。
        *   `| retriever`: 将提取出的问题字符串，通过管道符 `|` 直接传给我们在第1步创建的 `retriever`。
        *   `context=...`: 整个表达式的输出（即检索到的文档列表）会被赋值给一个新的键 `context`。
        *   **数据流**: 输入 `{"messages": [...]}` -> 经过这步后，数据流变为 `{"messages": [...], "context": [doc1, doc2, ...]}`。
    3.  `.assign(answer=document_chain)`:
        *   LCEL 会将上一步产生的**整个字典** (`{"messages": ..., "context": ...}`) 作为输入，传递给 `document_chain`。
        *   `document_chain` 正好需要 `messages` 和 `context` 这两个输入键，于是它完美地执行并生成答案。
        *   `answer=...`: `document_chain` 的输出被赋值给一个新的键 `answer`。
        *   **最终输出**: `{"messages": ..., "context": ..., "answer": "..."}`。

这个 `retrieval_chain` 现在可以一步到位地完成“接收问题 -> 检索文档 -> 生成答案”的全过程。

---

### 4. 查询转换 (Query transformation) - 让机器人理解对话

**问题**: 上面的链在处理**后续问题**时会失效。比如：
1.  用户: "LangSmith 如何帮助测试我的 LLM 应用？" (链工作得很好)
2.  用户: "**告诉我更多信息！**"

如果直接把 "告诉我更多信息！" 传给 `retriever`，它会因为这个查询本身没有明确的语义而检索到不相关的文档。

**解决方案**: 在检索之前，先用一个 LLM 将**结合了对话历史**的后续问题，**转换**成一个独立的、完整的查询。

**步骤 4.1: 创建查询转换链**

```python
query_transform_prompt = ChatPromptTemplate.from_messages([
    MessagesPlaceholder(variable_name="messages"),
    ("user", "Given the above conversation, generate a search query... Only respond with the query..."),
])

query_transformation_chain = query_transform_prompt | chat
```
*   **讲解**: 这个链的作用是接收完整的对话历史 `messages`，然后让 LLM 生成一个最适合用于搜索的、包含了所有上下文信息的新查询。例如，它会把 "告诉我更多信息！" 转换成 "LangSmith LLM application testing and evaluation" (LangSmith LLM 应用测试与评估)。

**步骤 4.2: 将查询转换整合进检索链**

我们使用 `RunnableBranch` 来实现条件逻辑：
*   **如果**这是对话的第一条消息，就直接用它来检索。
*   **否则**（如果这是后续问题），就先用查询转换链来生成新查询，再用新查询去检索。

```python
from langchain_core.runnables import RunnableBranch

query_transforming_retriever_chain = RunnableBranch(
    (
        # 条件：检查消息列表的长度是否为1
        lambda x: len(x.get("messages", [])) == 1,
        # 如果为真 (是第一条消息)，执行这个分支
        (lambda x: x["messages"][-1].content) | retriever,
    ),
    # 如果为假 (是后续消息)，执行这个分支
    query_transform_prompt | chat | StrOutputParser() | retriever,
)
```
*   **讲解**: `RunnableBranch` 就像一个 `if/else` 语句。它根据条件选择执行哪个子链。这样，我们就创建了一个更智能的检索器，它能处理上下文。

**步骤 4.3: 构建最终的对话式检索链**

最后，我们用这个新的、更智能的 `query_transforming_retriever_chain` 来替换之前简单的 `retriever`，构建最终的链。

```python
conversational_retrieval_chain = RunnablePassthrough.assign(
    context=query_transforming_retriever_chain, # <- 使用了带转换功能的检索器
).assign(
    answer=document_chain,
)
```
*   **讲解**: 结构和第3步完全一样，但 `context` 的生成过程变得更加智能和健壮，能够正确处理对话中的后续问题。

### 总结

这份文档通过一个完整的流程，展示了如何从零开始构建一个强大的、能够结合外部知识并理解对话上下文的 RAG 聊天机器人：
1.  **准备知识库**: 加载、切分、嵌入、存储文档，并创建一个 `retriever`。
2.  **构建问答核心**: 创建一个 `document_chain`，它知道如何利用上下文来回答问题。
3.  **处理对话上下文**: 创建一个 `query_transformation_chain` 来改写后续问题，使其变为独立的查询。
4.  **组装最终系统**: 使用 LCEL 将以上所有组件（智能检索器、问答核心）流畅地组合成一个 `conversational_retrieval_chain`。

最后提到的 `.stream()` 功能是 LCEL 的一大优势，它让这个复杂的链能够轻松实现流式输出，提升了用户体验。

## 补充

当然！您提的这两个问题是从初级 LangChain 用户迈向高级用户的必经之路。理解了这两点，您对 LangChain 表达式语言 (LCEL) 的掌握将提升一个台阶。

让我们逐一、深入地讲解。

---

### 问题一：`invoke` 的参数为什么从一句话变成了 `{"messages": [...]}`？

您观察得非常敏锐。这代表了从构建一个**简单的“单功能”链**到一个**复杂的、具备“多状态”能力的链**的转变。

#### 1. 简单的“单功能”链

在最基础的例子里，一个链可能长这样：

```python
prompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")
llm = ChatOpenAI()
simple_chain = prompt | llm
```

这个链的功能非常单一：接收一个 `topic`，生成一个笑话。所以，它的调用方式也很直接：

```python
simple_chain.invoke({"topic": "cats"}) 
# 或者如果prompt模板是 from_template("{input}")，就可以直接 invoke("cats")
```

这里的输入很简单，就是一个字符串或者一个只包含单个键的字典。因为链的需求就这么简单。

#### 2. 复杂的“多状态”链 (比如我们的 RAG 聊天机器人)

现在，我们再看您的 RAG 链。这个链需要处理的**信息状态**远比一个 `topic` 复杂：

*   **它需要知道用户的最新问题**：这是用来进行向量检索的核心内容。
*   **它需要知道完整的对话历史**：这是为了解决后续问题（比如用户说“告诉我更多”）。如果没有历史记录，`query_transformation` 这一步就无法工作。
*   **它需要知道检索到的上下文**：这是 `document_chain` 用来生成最终答案的依据。

为了管理这些复杂的状态，我们不能再用一个简单的字符串作为输入了。我们需要一个**结构化的数据格式**，而字典是最好的选择。

`{"messages": [Message1, Message2, ...]}` 就成了一种标准化的“对话状态”输入格式。

*   `messages` 这个键名是与提示模板中的 `MessagesPlaceholder(variable_name="messages")` 相对应的。
*   列表 `[...]` 的结构天然地适合存储有序的对话历史。
*   通过 `messages[-1]` 我们可以轻松获取最新的消息。

**结论**：当一个链需要处理多种信息（如历史、上下文、当前输入）时，它的 `invoke` 参数就需要是一个能够承载这些信息的结构化对象（通常是字典）。`{"messages": [...]}` 正是为对话式应用量身定做的标准输入格式。

---

### 问题二：`RunnablePassthrough` 到底是什么，为什么需要它？

`RunnablePassthrough` 是 LCEL 中一个看似简单却极其强大的组件。

#### 1. 它的核心功能

它的核心功能正如其名：**“直通车”**。它接收一个输入，然后**原封不动地将这个输入作为输出**。

```python
from langchain_core.runnables import RunnablePassthrough

passthrough = RunnablePassthrough()

# 你给它什么，它就输出什么
print(passthrough.invoke("hello")) # 输出: "hello"
print(passthrough.invoke({"a": 1})) # 输出: {"a": 1}
```

#### 2. 为什么要用一个“什么都不做”的组件？

这才是问题的关键。`RunnablePassthrough` 在 LCEL 中主要扮演两个至关重要的角色：

**角色一：作为数据管道的“起点”或“发起者”**

在您的代码中：
`retrieval_chain = RunnablePassthrough.assign(...)`

这里的 `RunnablePassthrough` 就是扮演这个角色。让我们分解一下：

1.  `.assign()` 方法的作用是在**传入的字典**基础上，增加新的键值对。
2.  那么问题来了，`.assign()` 的“传入的字典”从哪里来呢？
3.  它就来自于 `.assign()` 前面的那个组件。
4.  在这里，`RunnablePassthrough` 就是前面的组件。当您调用 `retrieval_chain.invoke({"messages": [...]})` 时，`RunnablePassthrough` 首先接收到这个字典，然后**原封不动地**把它传递给 `.assign()`。

所以，这里的 `RunnablePassthrough` 就像一个**启动信号**，它告诉链：“**请将 `invoke` 的原始输入，完整地传递给下一步**”。

如果没有它，链 ` .assign(context=parse_retriever_input | retriever)` 就不知道它的初始输入是什么。

**角色二：在并行处理中“复制”数据流**

这是 `RunnablePassthrough` 更常见的用法。当您需要将**同一个输入**用于多个不同的处理分支时，它就派上用场了。

想象一下，您需要一个链，它接收一个问题，然后同时：
*   返回原始问题。
*   返回问题的摘要。

您可以这样构建：
```python
from langchain_core.runnables import RunnablePassthrough
# 假设你有一个 summarizer_chain
# summarizer_chain = ... 

parallel_chain = {
    "original": RunnablePassthrough(), # 分支1
    "summary": summarizer_chain         # 分支2
}

# 调用
parallel_chain.invoke("LangChain Expression Language is a powerful way to compose chains.")
```

**执行流程**：
1.  `invoke` 的输入字符串被**同时**传递给字典里的两个键。
2.  对于 `"original"` 键：`RunnablePassthrough()` 接收到字符串，然后直接输出该字符串。
3.  对于 `"summary"` 键：`summarizer_chain` 接收到字符串，然后处理它并输出摘要。
4.  **最终输出**会是一个字典，包含了两个分支的结果：
    ```json
    {
      "original": "LangChain Expression Language is a powerful way to compose chains.",
      "summary": "LCEL is a powerful tool for chaining." 
    }
    ```

在这里，`RunnablePassthrough` 的作用就是开辟一个**“直通”分支**，确保原始数据能够在最终输出中保留，而不会被其他分支的处理过程所覆盖。

### 总结

*   **`invoke({"messages": [...]})`**: 是因为您的链变得复杂，需要一个结构化的输入来管理对话历史和当前问题，而不再是简单的“一问一答”。
*   **`RunnablePassthrough`**: 是一个“直通车”工具。在您的 RAG 链中，它被用作**起点**，负责捕获 `invoke` 的完整输入并将其传递给 `.assign()`，从而启动整个数据处理流程。在更复杂的并行链中，它则用来**复制数据**，确保原始输入可以在多个分支中被使用。