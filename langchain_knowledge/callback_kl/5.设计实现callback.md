—**如何从零开始创建你自己的自定义回调处理器 (Custom Callback Handler)**。

前面的文档讲解了“是什么”、“在哪里用”，而这篇文档则专注于“**怎么做**”。

---

### 核心思想：继承、覆盖、实现

创建自定义回调的整个过程，遵循了一个非常标准的面向对象编程 (OOP) 模式：

1.  **继承 (Inherit)**: 你创建一个新的 Python 类，让它继承自 LangChain 提供的基类 `BaseCallbackHandler` (或者异步的 `AsyncCallbackHandler`)。这一步让你写的类自动获得了成为一个合格 Callback Handler 的所有“骨架”。
2.  **覆盖 (Override)**: 在你的新类里，你选择性地重新实现（覆盖）基类中你感兴趣的事件方法。比如，你只关心 LLM 何时开始和结束，那么你就只需要在你的类里写 `on_llm_start` 和 `on_llm_end` 这两个方法。
3.  **实现 (Implement)**: 在你覆盖的方法内部，编写你希望在那个事件发生时执行的任何逻辑代码，比如打印日志、更新数据库、发送网络请求等。

---

### `BaseCallbackHandler`：你的“蓝图”或“契约”

`BaseCallbackHandler` 这个基类非常重要。它像一个“蓝图”，定义了所有 LangChain 可能触发的回调事件的**标准方法名称**和**参数签名**。

当你继承它时，你等于是在说：“我保证我的这个 Handler 会遵守 LangChain 的这套事件规则。” 你不需要实现所有的方法，因为基类已经为你提供了所有方法的空实现（它们什么也不做）。你只需要“填空”，实现你关心的那些即可。

---

### 代码 Demo 深度讲解

让我们来一步步分解并讲解文档中的代码示例，看看如何创建一个记录活动并能抛出错误的 Handler。

#### 1. 创建自定义 Handler - `MyCustomHandler`

这个 Handler 的目标是：
*   在 LLM 开始时，打印一条消息。
*   在 LLM 结束时，打印一条消息。
*   如果 LLM 出错，打印一条错误消息。

```python
from typing import Any, Dict, List, Union
from uuid import UUID

from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import AgentAction, AgentFinish, LLMResult

class MyCustomHandler(BaseCallbackHandler):
    """一个自定义的回调处理器，用于演示。"""

    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> None:
        """当 LLM run 开始时打印。"""
        print(f"LLM run started.")
        print("Prompts are:")
        for prompt in prompts:
            print(prompt)

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """当 LLM run 结束时打印。"""
        print(f"LLM run finished.")
        print("Generations were:")
        # response.generations 是一个列表的列表，
        # 外层列表对应多个 prompt 的情况
        # 内层列表对应一个 prompt 可能有多个候选生成结果
        for generations in response.generations:
            for generation in generations:
                print(generation.text)

    def on_llm_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> None:
        """当 LLM run 遇到错误时打印。"""
        # 这个方法非常重要，用于捕获和处理在 LLM 调用期间发生的异常
        print(f"Oh no, an LLM error occurred: {error}")

    # 除了 LLM，我们也可以实现其他事件，比如 on_chain_start, on_tool_end 等
    # 这里为了简洁省略了，但原理完全相同
```

**代码讲解**:

*   `class MyCustomHandler(BaseCallbackHandler):`: 这是第一步，**继承**。
*   `def on_llm_start(...)`: 这是**覆盖**。我们实现了自己的 `on_llm_start` 逻辑。
    *   `prompts: List[str]`: 这个参数非常有用，它让你能精确地看到**最终**被发送给 LLM 的完整提示是什么。
*   `def on_llm_end(...)`: 覆盖 `on_llm_end` 方法。
    *   `response: LLMResult`: 这个参数是一个结构化对象，包含了 LLM 返回的所有信息，包括生成的文本、Token 使用情况（如果模型提供的话）等。我们通过 `response.generations` 来访问生成的文本。
*   `def on_llm_error(...)`: 覆盖 `on_llm_error` 方法。
    *   `error: Union[Exception, KeyboardInterrupt]`: 这个参数会接收到导致 LLM 失败的那个异常对象。这对于调试和构建健壮的应用至关重要，你可以在这里记录错误、发送警报或执行重试逻辑。

#### 2. 在代码中使用这个 Handler

现在我们已经定义好了“蓝图”的实现，就可以像使用任何其他 Handler 一样使用它了。

```python
from langchain_openai import OpenAI

# 创建一个 LLM 实例，并在构造函数中传入我们的自定义 Handler
# 这意味着这个 llm 实例的所有调用都会被 MyCustomHandler 监听到
llm = OpenAI(callbacks=[MyCustomHandler()])

# --- 运行一个成功的调用 ---
print("--- Running a successful call ---")
llm.invoke("What is a good name for a company that makes colorful socks?")

print("\n" + "="*40 + "\n")

# --- 模拟一个失败的调用 ---
# 我们通过传递一个不存在的模型名称来故意触发一个 API 错误
try:
    print("--- Running a failing call ---")
    bad_llm = OpenAI(model_name="non-existent-model", callbacks=[MyCustomHandler()])
    bad_llm.invoke("This will fail")
except Exception as e:
    # 尽管这里有一个 try/except 块捕获了异常，
    # 但我们的 on_llm_error 回调仍然会在异常被抛出之前触发！
    print(f"\nCaught exception in main code: {e}")
```

#### 3. 输出分析

**成功调用的输出**:
```
--- Running a successful call ---
LLM run started.
Prompts are:
What is a good name for a company that makes colorful socks?
LLM run finished.
Generations were:

Socktastic!
```
*   这清晰地展示了 `on_llm_start` 和 `on_llm_end` 被依次成功调用。

**失败调用的输出**:
```
--- Running a failing call ---
LLM run started.
Prompts are:
This will fail
Oh no, an LLM error occurred: The model `non-existent-model` does not exist... (后面是详细的错误信息)

Caught exception in main code: The model `non-existent-model` does not exist...
```
*   这里我们看到，`on_llm_start` 首先被调用。
*   接着，在 OpenAI 库内部尝试 API 调用并失败时，**`on_llm_error` 被触发了**，打印出了 "Oh no, an LLM error occurred"。
*   最后，这个异常才被抛出到我们的主代码中，并被 `try/except` 块捕获。

### 为什么要创建自定义 Handler？—— 实际应用场景

打印 `print` 只是为了演示，自定义 Handler 的真正威力在于实现真实世界的功能：

1.  **详细日志记录**: 将所有 LLM 的输入、输出、错误和 Token 消耗记录到一个文件或数据库中，用于审计和分析。
2.  **UI 更新**: 在一个 Web 应用中，使用 `on_llm_start` 来显示“机器人正在输入...”，使用 `on_llm_end` 或 `on_new_token` 来更新界面。
3.  **数据流式传输**: 实现 `on_llm_new_token` 方法，通过 WebSocket 将 LLM 生成的每一个词块实时发送到前端。
4.  **成本和性能监控**: 在 `on_llm_end` 中，从 `response` 对象中提取 Token 使用数据，并将其发送到监控系统（如 Prometheus, Datadog）来追踪成本和延迟。
5.  **错误报警**: 在 `on_llm_error` 或 `on_chain_error` 中，连接到报警系统（如 Sentry, PagerDuty），在应用出现严重错误时立即通知开发人员。

通过创建自定义回调，你将 LangChain 从一个黑盒变成了一个完全透明和可观测的白盒，可以将其深度集成到你自己的应用架构中。